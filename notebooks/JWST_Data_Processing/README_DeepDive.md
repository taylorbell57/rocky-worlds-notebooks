
## Preface

A collection of Eureka! Control Files (.ecf files) and a Eureka! Parameter File (.epf file) are copied into the cells of this notebook. These parameters are setup with generally reasonable and performant settings that should give a good first analysis of the data from which you can further optimize the reduction.

For the deep-dive processing, the Stage 1-5 Eureka! Parameter Files will need to be adjusted to produce an optimal and robust reduction.

All told, running Stages 1--5 for a single aperture+annulus pair will take ~30 minutes. Likely you'll want to explore many aperture+annulus pairs (the notebook is setup to automatically do this) which can result in many-hour analysis runs.

## Order of operations

Choose one Stage 1 setup. Run a suite of different S3 settings. Run Stage 4 on that suite. Initially run Stage 5 on just one reduction to make sure your S5 settings can produce a good fit. Once you know you can get a good fit out of S5, re-run S5 with the full suite of reductions. Compare reductions using the code near the bottom to choose which setting seems optimal.

Then make a copy of your notebook and run Stages 1-5 with different S1 settings but only your preferred S3 aperture+annulus setting. This will help to avoid enormous computational effort of trying to do a grid over many S1 settings and many S3 settings.

## Context behind the Stage 1 ECF

Some of the parameters that might be worth varying are as follows:

* `maximum_cores`: If you're running this notebook on STScI functional computers and/or you want to limit the CPU usage of the stage, you can set this to an integer number of cores or one of `'none'` (for single-threaded operations), `'quarter'`, `'half'`, or `'all'`.
* `skip_emicorr`: It could be worth changing this between `True`/`False`, but this likely won't have a very large impact. Some preliminary testing has suggested that setting it to `False` may give better results.
* `skip_firstframe` & `skip_lastframe`: These might be worth changing between `True`/`False`. In theory it'd be best to set both to `False` (which removes the first and last groups, respectively), but for integrations with small numbers of groups it might be worth setting one or both to `True`. It is hard to say for sure without just trying different combinations.
* `skip_jump` & `jump_rejection_threshold`: The current jump step correction algorithm tends to suffer from quite high rates of false positives, so increasing `jump_rejection_threshold` beyond the `jwst` pipeline default of `4.0` to values like `8.0` or `10.0` might be worth trying, or potentially even completely skipping the jump step by setting `skip_jump` to `True`.
* `skip_rscd`: This step helps to remove RSCD-related effects at the start of up-the-ramp reads. For integrations with small numbers of groups, this step is automatically skipped. However, even for integrations with intermediate numbers of groups, it may be worth skipping this step (setting `skip_rscd` to `True`) as the observations may be more photon-noise limited than RSCD-noise limited. Double-check whether or not the RSCD step was already skipped by default though before changing `skip_rscd` to `True`.
* `skip_dark_current` & `skip_refpix`: By default these two steps are run for MIRI TSO observations and are likely best left to run (set both skip parameters to `False`), but it's maybe possible that better results could be obtained by experimenting with setting the skip parameters to `True`.

For more information on the options available in Stage 1, visit https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-1

## Context behind the Stage 3 ECF

Some of the parameters that might be worth varying are as follows:

* `ncpu`: If you want to control the CPU usage of the stage, you can set this to an integer number of cores.
* `xwindow` and `ywindow`: These control how much of the subarray is cutout and stored in RAM. This parameter is generally unimportant so long as your background subtraction annulus fits entirely within your cutout and so long as you're not limited by your amount of RAM
* `ff_outlier` & `bg_thresh`: These parameters control a second round of cosmic ray rejection. `bg_thresh` is formatted as a list of sigma thresholds that will be interated through when looking for outliers along the time-axis for each individual pixel. Generally `[5,5]` should work well and will do two rounds of 5-sigma outlier removal, but if there are annoying outliers that remain you can try using smaller threshold values, and if the step removes too many pixels then you can try using higher thresholds. `ff_outlier` controls whether this per-pixel outlier rejection is done for only the pixels outside of the source aperture (`ff_outlier=False`) or for the full frame (`ff_outlier=True`). In general, you're likely to get cleaner lightcurves if you set `ff_outlier=True`, but you must be careful not to do this with deep transits/eclipses as doing so can end up biasing your final results.
* `phot_method`: This sets the photometric extraction method, with options including `photutils` (which is generally fast and effective), `poet` (which is generally similar to `photutils` but does support hexagonal apertures if desired), or `optimal` (where flux-weighted optimal photometric extraction is used). For especially faint or background-limited observations, the `optimal` setting might outperform the `photutils` option.
* `aperture_edge`: For `phot_method` settings of `photutils` and `optimal` your options are `'center'` (pixel is included only if its center lies within the aperture), or `'exact'` (pixel is weighted by the fractional area that lies within the aperture). Often `'exact'` works best, but not always. For a `phot_method` setting of `poet`, only the `'center'` option is available.
* `aperture_shape`: This parameter controls both the object aperture shape and the sky annulus shape. If `phot_method` is `'photutils'` or `'optimal'`, your options are `'circle'`, `'ellipse'`, or `'rectangle'`. If `phot_method` is `'poet'`, your options are `'circle'` or `'hexagon'`.
* `moving_centroid`: This boolean parameter. If set to `False` (generally recommended), the aperture will stay fixed on the median centroid location, or if set to `True` then the aperture will track the moving centroid (may help in cases where the star moves a lot across the detector).
* `photap`, `skyin`, and `skywidth`: These parameters control the size of photometry aperture radius, inner sky annulus radius, and width of the sky annulus in units of pixels. You want `photap` to be large enough to capture the Airy disk (generally \~5 pixels) but sometimes you want it to be large enough to also capture the first Airy ring (\~11 pixels or so). It is important that the `skyin` setting be set large enough to avoid most/all of the first Airy ring while still being close enough to the host star to probe the background close to the source aperture. It is harder to give a hard-and-fast guide for the `skywidth`, but generally you want it to be large enough to avoid induced noise from computing the background from only a small number of pixels while also making it small enough to not include noisy pixels, nearby stars/galaxies, or unrepresentative regions of the background. You can also interatively try different combinations of `photap`, `skyin`, and `skywidth` by making one or more of the variables list with the format `[Start, Stop, Step]`; this will give you sizes ranging from Start to Stop (inclusively) in steps of size Step. For example, `[10,14,2]` tries 10, 12, and 14, but `[10,15,2]` still tries 10, 12, and 14. If more than one of `photap`, `skyin`, and/or `skywidth` are lists, all combinations of the three will be attempted. For example, `photap=[4,6,1]`, `skyin=[16,18,2]`, and `skywidth=30` would result in `[ap4_bg16_36,ap5_bg16_36,ap6_bg16_36,ap4_bg18_38,ap5_bg18_38,ap6_bg18_38]`. If `aperture_shape` is `'ellipse'` or `'rectangle'`, then in addition to `photap` you'll also have the option to use `photap_b` (semi-major axis of the aperture along the y-axis) and `photap_theta` (the rotation angle of photometry aperture in degrees above the positive x-axis; the aperture rotates about its center). The shape and aspect ratio of the sky annulus will be the same as the shape and aspect ratio of the source aperture.

For more information on the options available in Stage 3, visit https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-3


## Context behind the Stage 4 ECF

If after running Stage 4 you notice there were too many or too few clipped outliers, then you can adjust the `sigma` and/or `box_width` parameters here. You can likely rely on what was found in the quick-look analysis notebook though.

You may also want/need to adjust `allapers`, which is a boolean to determine whether Stage 4 is run on all the apertures considered in Stage 3. If `False`, it will just use the most recent output in the input directory. For example, if you only want to run Stage 4 on a specific aperture+annulus pair from Stage 3, make sure to update your `inputdir` path to be more precise (i.e., add `ap#_bg#_#/` to the end of the path to specify which particular aperture + annulus pairing)

## Context behind the Stage 5 ECF

For your very first time running the Stage 5 fits, it is strongly recommended that you set `allapers` to `False` so that you can assess how well your fitting setup models the observations. This will let you quickly find out if the model is not able to produce a good fit to your observations, and then you can quickly iterate until you are able to produce a good fit. Once you're able to get a good fit, then you can set `allapers` to `True` which will then perform the fit for all the different apertures you considered in Stage 3 & 4.

There are only a couple attributes of the Stage 5 which you may need to change:

* `ncpu`: Similar with Stages 1 & 3, adjust the number of CPU threads if needed.
* `allapers`: Boolean to determine whether Stage 5 is run on all the apertures considered in Stage 4. If False, will just use the most recent output in the input directory.
* `manual_clip`: A list of lists specifying the start and end integration numbers for manual removal. E.g., to remove the first 20 data points specify `[[0,20]]`, and to also remove the last 20 data points specify `[[0,20],[-20,None]]`. If you want to clip the 10th integration, this would be index `9` since python uses zero-indexing. And the `manual_clip` start and end values are used to slice a numpy array, so they follow the same convention of inclusive start index and exclusive end index. In other words, to trim the 10th integrations, you would set manual_clip to `[[9,10]]`. The default in the template ECF will remove the first 50 integrations which will likely remove most of the worst of the initial exponential ramp, but you should tune this if needed.
* `run_myfuncs`: Determines the astrophysical and systematics models used in the Stage 5 fitting. For standard numpy functions, this can be one or more (separated by commas) of the following: `[batman_tr, batman_ecl, catwoman_tr, fleck_tr, poet_tr, poet_ecl, sinusoid_pc, quasilambert_pc, expramp, hstramp, polynomial, step, xpos, ypos, xwidth, ywidth, lorentzian, damped_osc, GP]`. The `poet_tr` and `poet_ecl` models assume a symmetric transit shape and, thus, are best-suited for planets with small eccentricities (e < 0.2). POET has a fast implementation of the 4-parameter limb darkening model that is valid for small planets (Rp/Rs < 0.1). In general for MIRI photometry, you will want to use `['batman_ecl', 'polynomial', 'expramp', 'xpos', 'ypos', 'xwidth', 'ywidth', 'GP']` (with the possible exception of the GP model which may not always be needed).
* If you are using the GP model, you can just leave the `GP inputs` section of the S5 ECF as-is (the MatÃ©rn-3/2 kernel from `celerite` generally works just fine).
* The dynesty fitting parameters are setup such that you should get decent and robust results. If you want your corner plots to look more filled in, you can set `run_nlive` to some integer that is larger than `ndim * (ndim + 1) // 2` (where `ndim` is the number of fitted parameters), but in gneral that shouldn't be necessary.

For more information on the options available in Stage 5, visit https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-5.
For more information about the models and the fitted parameters that correspond to each of these models, see https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-5-fit-parameters.


## Context behind the Stage 5 EPF

For all observations, we must specify some important details that will determine the setup of our fitted model including specifying any parameters we want to fix to a specific value and specifying Bayesian priors on any parameters we want fitted. These model settings are specified through a Eureka! Parameter File (EPF; file extension of .epf); we will specify the contents of that file in the next cell of the notebook and then write that to a file.

In particular, you will need to specify the planet-to-star radius ratio (`rp`; unitless), planet-to-star flux ratio (`fp`; unitless; also called the eclipse depth), orbital period (`per`; units of days), expected mid-time of the eclipse (`t_secondary`; units of BMJD_TDB), orbital inclination (`inc`; units of degrees), orbital semi-major axis to stellar radius ratio (`a`; unitless). If relevant to your particular system, you may also need to fit for the eccentricity (`ecc`; unitless) and longitude of periastron (`w`; units of degrees), or alternatively fit for `ecosw` and `esinw` which allow for better behaved fits with less of a bias towards `e > 0`.

The JWST Scheduling Team of the CIT provides the JWST Data Analysis Team of the CIT with the expected values for each of these parameters. You will find these on Jira and shouldn't need to change them beyond what is specified on Jira. However, below is some added context about how that orbital parameter setup was chosen.
For our deep-dive analysis, we will initially "fix" all astrophysical parameters to the values provided by the observation planning team, with the exception of `fp` (the eclipse depth, which is our main parameter of interest) and `t_secondary` (the time of secondary eclipse, which will be one of the most uncertain orbital parameters, a priori). To fix parameters, you must simply specify their expected values in the second column of the EPF file, under the column header "Value". To allow `fp` and `t_secondary` to vary within some reasonable level of uncertainty, we must provide Bayesian priors to the model, and we will base these on the expectations provided by the observation planning team. In particular, we will set the "Value" and "PriorPar1" columns to the expected values provided by the observation planning team; the "Value" column sets the starting point of your model fit, while the "PriorPar1" column sets the mean of the Gaussian prior (which "PriorType" is set to "N", which stands for "Normal"). We will set the "PriorPar2" column of the `t_secondary` parameter to the 1-sigma uncertainty on the mid-eclipse time provided by the observation planning team. However, for `fp` we want to minimally bias our conclusions, so we will set "PriorPar2" to be `2000e-6` which will allow for a very broad range of eclipse depths to be considered by the fitter.

All other variables starting from the "Systematic variables" header have already been setup to have reasonable starting points and priors that should work well enough for all of our observations. However, if you find that your fits are being noticeably restricted by any of these priors, consider adjusting them while keeping the mindset of making the priors "minimially informative" (i.e. where feasible keep the priors broad and let the data constrain the values they should take).

## Advice on how to optimize your reduction

When choosing how "best" to fit your lightcurve, there are several things to keep in mind. It is important to remember that we are not merely looking for the "best" results but also "robust" results, where reasonable changes in reduction or fitting choices result in reasonable levels of agreement with your "best" reduction/fit.

Finding the reduction that minimizes the value of `scatter_mult` or the uncertainty on `fp` can sometimes be a good metric (the more precise your result the better), but it can also result in some risky biases. For example, unless you're simultaneously fitting for a Gaussian Process while fitting your lightcurve, the uncertainties on `fp` and the value of `scatter_mult` do not account for any residual "red noise" (aka temporally correlated noise) in your lightcurve residuals. As a result, a horrible looking fit might still give very low uncertainties on `fp` which are not reflective of the true uncertainties. You should also double-check that the "Reduced Chi-squared" value printed out after the fit completed is quite close to 1.0; if not, this is likely indicative of a failed fit as the `scatter_mult` parameter should have been able to inflate the uncertainties on each integration such that the reduced chi-squared value of the fit was 1.0. A visual representation of how close the chi-squared value is to 1.0 is in the "Residuals/Uncertainty" histogram plot, in which the observed histogram in blue should look quite close to the theoretical histogram shown in black. When comparing different model fits to the same data, you can also use the Bayesian Evidence (`logz`) estimated by dynesty, where larger `logz` values indicate a preferred model (though maybe not statistically significantly preferred, depending on how large the change in `logz` is); again though, keep in mind that this Bayesian Evidence doesn't account for residual red noise.

An important metric to help you determine whether there is residual red noise in your fit is the Allan Variance Plot (which has a plot title of "Correlated Noise"). This plot shows how the RMS of the lightcurve residuals decreases with increasing temporal binning. With perfectly Gaussian noise, the RMS of the binned residuals should fall with the square-root of the bin size; this trend is shown with a solid red line. In the presence of temporally-correlated noise, the RMS of the residuals will deviate above this red line at timescales where there is red noise; the observed noise levels at different levels of binning is shown with a solid black line. There is also some amount of uncertainty in the observed RMS, which is shown with a light grey envelope around the solid black line. If the black line and its envelope rise significantly above the red line, this indicates that there is significant amounts of residual red noise in your lightcurve. Ideally the source of this unmodelled red noise will by visible to you by-eye in the lightcurve residuals; if so, you can try adjusting your model and priors in your Stage 5 ECF and EPF (e.g., allow for a steeper exponential ramp, allow for two exponential ramps instead of just one, allow for a quadratic in time).

If, however, you cannot identify a simple functional form for the correlated noise in the residuals, you can instead use a Gaussian Process (GP) which is simultaneously a very powerful and a very dangerous tool. If used properly, a GP is able to marginalize over all possible functional forms for the red noise while appropriately inflating the uncertainties on the parameters of interest to account for correlations between the red noise model and the eclipse model. However, GPs can also be dangerous as (depending on your GP model priors) they can end up perfectly modelling the entire lightcurve without any astrophysical model at all; this is especially risky when dealing with small eclipse signals where it may not be immediately obvious that the GP is fighting with the eclipse model. As a result, I strongly recommend you do your best to model the lightcurve without a GP and then only include a GP if you've largely optimized the setup of all your priors and other model choices, and I recommend being very cautious if reducing the lower-limit of the uniform prior (in the `PriorPar1` column of the EPF) on the value of `m` which is the GP lengthscale. If allowed to become too small, the GP can overfit the data and try to model white Gaussian noise; to make sure the GP lengthscale isn't too small, look at the middle panel of the lightcurve plot which is titled "GP Term (ppm)", and make sure that the fitted GP model looks like a smoothly varying function and not a scatter plot.