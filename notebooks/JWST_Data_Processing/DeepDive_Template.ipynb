{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f19463d-b555-49f1-9ef0-8afd8043c9e3",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7c1c8-4b88-4fc7-8a33-a11b624a7c16",
   "metadata": {},
   "source": [
    "# RW-DDT Deep-Dive Analysis Template Notebook\n",
    "\n",
    "**Authors**: Taylor James Bell (ESA/AURA for STScI)<br>\n",
    "**Last Updated**: August 04, 2025<br>\n",
    "**jwst Pipeline Version**: 1.18.0 (Build 11.3)<br>\n",
    "**Eureka! Pipeline Version**: https://github.com/kevin218/Eureka/tree/tjb_rwddt\n",
    "\n",
    "Note that additional contextual information can be found in `README_DeepDive.md`\n",
    "\n",
    "**Purpose**:<br/>\n",
    "\n",
    "A collection of Eureka! Control Files (.ecf files) and a Eureka! Parameter File (.epf file) are contained within the DeepDive_setup folder provided on Box. These files are setup with generally reasonable and performant settings that should give a good first-look at the data to determine whether or not the observations were successful. Deep-dive analyses will start with _uncal.fits files downloaded from MAST and will then be processed through Stages 1--5 of Eureka!.\n",
    "\n",
    "When doing aperture optimization or experimenting with different reduction parameters, you will have to repeat Stage 3 (and possibly Stages 4-5) for each reduction setting which can end up taking quite a while. To try to reduce the amount of human effort required to optimize your reduction settings, some amount of the aperture optimization is setup to be automatically done, and there are useful comments throughout this notebook which will help you experiment with other reduction settings that can't be automatically varied as easily.\n",
    "\n",
    "**Data**:<br/>\n",
    "This notebook assumes the uncal files have already been downloaded from MAST using the `rocky-worlds-utils/download_JWST.py` script.\n",
    "\n",
    "**JWST pipeline version and CRDS context**:<br/>\n",
    "This notebook was written for the calibration pipeline version given above and uses the context associated with this version of the JWST Calibration Pipeline. Information about this and other contexts can be found in the JWST Calibration Reference Data System (CRDS) [server]((https://jwst-crds.stsci.edu/)). If you use different pipeline\n",
    "versions, please refer to the table [here](https://jwst-crds.stsci.edu/display_build_contexts/) to determine what context to use. To learn more about the differences for the pipeline, read the relevant [documentation](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/jwst-operations-pipeline-build-information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dc5f1-3275-4545-8a7c-9c30d5489446",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Table of Contents\n",
    "- [0. Importing the required components](#0.-Importing-the-required-components)\n",
    "  - [0.1 Define your eventlabel and top directory](#0.1-Define-your-eventlabel-and-top-directory)\n",
    "- [1. Stage 1](#1.-Stage-1)\n",
    "- [2. Stage 2](#2.-Stage-2)\n",
    "- [3. Stage 3 - Pixels to Lightcurve](#3.-Stage-3---Pixels-to-Lightcurve)\n",
    "- [4. Stage 4 - Removing time-series outliers](#4.-Stage-4---Removing-time-series-outliers)\n",
    "- [5. Stage 5 - Fitting the lightcurve](#5.-Stage-5---Fitting-the-lightcurve)\n",
    "- [6. Comparing results for different aperture sizes](#6.-Comparing-results-for-different-aperture-sizes)\n",
    "- [7. Choosing a final fiducial reduction](#7.-Choosing-a-final-fiducial-reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef799f6-259c-4f7f-8beb-1bf8b9b3cb6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Importing the required components\n",
    "\n",
    "There should be no need to change any of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2bd1f-de2c-48d5-a150-918182ee0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a bunch of Eureka! components\n",
    "import eureka.lib.plots\n",
    "from eureka.lib.manageevent import findevent\n",
    "from eureka.lib.readECF import MetaClass\n",
    "import eureka.S1_detector_processing.s1_process as s1\n",
    "import eureka.S2_calibrations.s2_calibrate as s2\n",
    "import eureka.S3_data_reduction.s3_reduce as s3\n",
    "import eureka.S4_generate_lightcurves.s4_genLC as s4\n",
    "import eureka.S5_lightcurve_fitting.s5_fit as s5\n",
    "\n",
    "# Set up some parameters to make plots look nicer. You can set usetex=True if you have LaTeX installed\n",
    "eureka.lib.plots.set_rc(style='eureka', usetex=False, filetype='.png')\n",
    "\n",
    "# Some imports to interact with outputs within the Jupyter notebook\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb96fb2-345d-4d86-90b9-fef338bdf4ef",
   "metadata": {},
   "source": [
    "### 0.1 Define your eventlabel and top directory\n",
    "\n",
    "Next, we need to choose a short, meaningful label (without spaces) that describes the data we're currently working on. This eventlabel will determine will give nicknames to all your output folders and files.\n",
    "\n",
    "We also need to tell the notebook where all our data is going to be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd379f0e-1ab2-4ec1-9d5f-80d1f2a6312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter in a custom eventlabel that will be used to distinguish the outputs\n",
    "# from all subsequent processing\n",
    "eventlabel = ''\n",
    "\n",
    "# Specify here the top directory that will contain all ingested and output files\n",
    "topdir = '/home/rwddt' ## <- ENTER YOUR TOPDIR HERE (leave at /home/rwddt if using Docker)\n",
    "\n",
    "# Specify your analysis name here (analysis if using Docker, Analysis_A or Analysis_B otherwise)\n",
    "analysis_name = 'analysis'\n",
    "\n",
    "# A full Eureka! run through all five stages with many different aperture+annulus pairs can take many hours.\n",
    "# If we want to just grab the results from previous runs and carry out comparisons, you can set process=False\n",
    "# below to skip the Eureka! processing portions of the notebook.\n",
    "process = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf1617-7d2d-476a-a696-c96aeece1bfa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Stage 1\n",
    "\n",
    "### 1.1 Setting up the Stage 1 ECF\n",
    "\n",
    "A good starting point for your deep-dive analysis has already been populated within the template Stage 1 ECF file, so first we'll load that in and look at the default settings\n",
    "\n",
    "Parameters that might be worth varying are as documented in the `README_DeepDive.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff822c95-24f6-4c7d-87f6-b39ce06f45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_ecf_contents = f\"\"\"# Eureka! Control File for Stage 1: Detector Processing\n",
    "\n",
    "# Stage 1 Documentation: https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-1\n",
    "\n",
    "suffix                    uncal\n",
    "pmap                      1364         # Setting a fixed pmap value to ensure consistency in reductions\n",
    "\n",
    "maximum_cores             'half'       # Options are 'none', 'quarter', 'half', 'all', or any integer\n",
    "\n",
    "# Pipeline stages\n",
    "skip_emicorr              False        #### FINDME: Might be worth turning on/off\n",
    "skip_saturation           False\n",
    "skip_firstframe           False        #### FINDME: Might be worth turning on/off\n",
    "skip_lastframe            False        #### FINDME: Might be worth turning on/off\n",
    "skip_reset                False\n",
    "skip_linearity            False\n",
    "skip_rscd                 False        #### FINDME: Might be worth turning on/off\n",
    "skip_dark_current         False        #### FINDME: Might be worth turning on/off\n",
    "skip_refpix               False        #### FINDME: Might be worth turning on/off\n",
    "skip_jump                 False\n",
    "skip_clean_flicker_noise  True         #### FINDME: Might be worth turning on/off\n",
    "\n",
    "#Pipeline stages parameters\n",
    "jump_rejection_threshold  8.0          #### FINDME: Might be worth trying different values\n",
    "\n",
    "# Diagnostics\n",
    "isplots_S1                3\n",
    "nplots                    5\n",
    "hide_plots                True\n",
    "verbose                   True\n",
    "\n",
    "# Project directory\n",
    "topdir              {topdir}\n",
    "\n",
    "# Directories relative to topdir\n",
    "inputdir            Uncalibrated\n",
    "outputdir           {analysis_name}/DeepDive/Stage1\n",
    "\"\"\"\n",
    "\n",
    "# This will save the ECF as a file that the next cell can read-in\n",
    "with open(f'./S1_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s1_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1ce78-2b4e-4315-9b0d-c0555edcbdfd",
   "metadata": {},
   "source": [
    "### 1.2 Running Stage 1\n",
    "\n",
    "Here we run the Eureka! Stage 1 pipeline using the settings we defined above. This should take several minutes (~11 minutes for a TRAPPIST-1b eclipse observation on a laptop with an M3 Max CPU), but that will depend on the data volume of the observation you're working on and the specifics of your CPU\n",
    "\n",
    "If you previously ran Stage 1 and want to re-use those outputs, you can just comment-out the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cfef6-7412-4e83-ab91-8ef81f54f0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if process:\n",
    "    s1_meta = s1.rampfitJWST(eventlabel)\n",
    "else:\n",
    "    # If not running S1, automatically grab the latest MetaData\n",
    "    temp_meta = MetaClass()\n",
    "    temp_meta.eventlabel = eventlabel\n",
    "    temp_meta.topdir = topdir\n",
    "    temp_meta.inputdir = f'{topdir}/{analysis_name}/DeepDive/Stage1/'\n",
    "    s1_meta, *_ = findevent(temp_meta, 'S1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c74ed5-b828-4f75-92f5-708bcc0f14bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Stage 2\n",
    "\n",
    "### 2.1 Setting up the Stage 2 ECF\n",
    "\n",
    "For deep-dive lightcurve analyses, there is no need to change any of the Stage 2 settings beyond what is provided in the template, so we'll just read those defaults in.\n",
    "\n",
    "By default, the most recently created Stage 1 output located in `s2_meta.inputdir` will be used as the input for your Stage 2. If you instead want to use a specific Stage 1 output, you'll need to update your `inputdir` setting to point to the specific Stage 1 output folder you want to work on (e.g., `'Stage1/S1_2025-02-28_trappist1b_eclipse4_run1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad2700-1fdc-4ab2-8606-0032f61f469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_ecf_contents = f\"\"\"# Eureka! Control File for Stage 2: Data Reduction\n",
    "\n",
    "# Stage 2 Documentation: https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-2\n",
    "\n",
    "pmap\t\t\t\t1364\n",
    "\n",
    "skip_flat_field     False       #### FINDME: Might be worth turning on/off\n",
    "skip_photom         True        # Strongly recommended to skip (unless doing flux calibrated photometry) in order to get better uncertainties out of Stage 3.\n",
    "\n",
    "# Project directory\n",
    "topdir              {topdir}\n",
    "\n",
    "# Directories relative to topdir\n",
    "inputdir            {analysis_name}/DeepDive/Stage1\n",
    "outputdir           {analysis_name}/DeepDive/Stage2\n",
    "\"\"\"\n",
    "\n",
    "# This will save the ECF as a file that the next cell can read-in\n",
    "with open(f'./S2_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s2_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58c4c3-62b9-4dc1-847d-99986f09e74e",
   "metadata": {},
   "source": [
    "### 2.2 Running Stage 2\n",
    "\n",
    "Here we run the Eureka! Stage 2 pipeline using the settings we defined above. This should take <1 minute, but that will depend on the data volume of the observation you're working on and the specifics of your CPU\n",
    "\n",
    "If you previously ran Stage 2 and want to re-use those outputs, you can just comment-out the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348819af-e2ba-491a-becb-9c2926fd06a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if process:\n",
    "    s2_meta = s2.calibrateJWST(eventlabel)\n",
    "else:\n",
    "    # If not running S2, automatically grab the latest MetaData\n",
    "    temp_meta = MetaClass()\n",
    "    temp_meta.eventlabel = eventlabel\n",
    "    temp_meta.topdir = topdir\n",
    "    temp_meta.inputdir = f'{topdir}/{analysis_name}/DeepDive/Stage2/'\n",
    "    s2_meta, *_ = findevent(temp_meta, 'S2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a00ed-252c-4edf-b4ba-e1929b81c0e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Stage 3 - Pixels to Lightcurve\n",
    "\n",
    "Stage 3 performs background subtraction and optimal spectral extraction. This will generate time series of 1D spectra.\n",
    "\n",
    "### 3.1 Setting up the Stage 3 ECF\n",
    "\n",
    "Parameters that might be worth varying are documented in the `README_DeepDive.md` file.\n",
    "\n",
    "By default, the most recently created Stage 2 output located in `s3_meta.inputdir` will be used as the input for your Stage 3. If you instead want to use a specific Stage 2 output, you'll need to update your `inputdir` setting to point to the specific Stage 2 output folder you want to work on (e.g., `'Stage2/S2_2025-02-28_trappist1b_eclipse4_run1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ee5a9-140d-444a-b336-9320fc196e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ecf_contents = f\"\"\"# Eureka! Control File for Stage 3: Data Reduction\n",
    "\n",
    "# Stage 3 Documentation: https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-3\n",
    "\n",
    "ncpu            16\n",
    "nfiles          200\n",
    "max_memory      1.5\n",
    "indep_batches   False       # Independently treat each batch of files? Strongly recommended to leave this as False unless you have a clear reason to set it to True.\n",
    "suffix          calints\n",
    "\n",
    "calibrated_spectra  False   # Set True to generate flux-calibrated spectra/photometry in mJy\n",
    "                            # Set False to convert to electrons\n",
    "pmap            1364\n",
    "\n",
    "# Subarray region of interest\n",
    "ywindow         None\t    # Vertical axis as seen in DS9\n",
    "xwindow         None        # Horizontal axis as seen in DS9\n",
    "dqmask          True\n",
    "\n",
    "# Background parameters\n",
    "ff_outlier      True        # Set False to use only background region (recommended for deep transits)\n",
    "                            # Set True to use full frame (works well for shallow transits/eclipses)\n",
    "bg_thresh       [5,5]\n",
    "interp_method   linear      # Interpolate bad pixels. Options: None (if no interpolation should be performed), linear, nearest, cubic\n",
    "\n",
    "# Centroiding parameters\n",
    "centroid_method mgmc        # Method used for centroiding. Options: mgmc, fgc\n",
    "ctr_guess\t\tfits    \t# Initial guess of centroid position. If None, will first perform centroiding on whole frame (can sometimes fail)\n",
    "ctr_cutout_size 10          # Cutoff size all around the centroid after the coarse centroid calculation or first centroid guess when using the mgmc method.\n",
    "centroid_tech   com         # (mgmc method param) Technique used for centroiding. Options: com, 1dg, 2dg\n",
    "gauss_frame     15          # (mgmc method param) Half-width away from second centroid guess to include in centroiding map for gaussian widths. Recommend ~15 for MIRI photometry.\n",
    "\n",
    "# Photometric extraction parameters\n",
    "phot_method     photutils   # photutils (aperture photometry using photutils), poet (aperture photometry using code from POET), or optimal (for optimal photometric extraction)\n",
    "aperture_edge   exact       # center (pixel is included only if its center lies within the aperture), or exact (pixel is weighted by the fractional area that lies within the aperture)\n",
    "aperture_shape  circle      # If phot_method is photutils or optimal: circle, ellipse, or rectangle. If phot_utils is poet: circle or hexagon. Used to set both the object aperture shape and the sky annulus shape\n",
    "moving_centroid False       # Boolean: False if the aperture should stay fixed on the median centroid location (recommended), or True if the aperture should track the moving centroid\n",
    "skip_apphot_bg  False       # Skips the background subtraction during the aperture photometry step\n",
    "photap          [4,11,1]    # Size of photometry aperture radius in pixels\n",
    "skyin           [14,26,4]   # Inner sky annulus edge, in pixels\n",
    "skywidth        [10,30,10]  # Width of the sky annulus, in pixels\n",
    "\n",
    "# Diagnostics\n",
    "isplots_S3      3\n",
    "nplots          3\n",
    "hide_plots      False\n",
    "save_output     True\n",
    "save_fluxdata   False\n",
    "verbose         True\n",
    "\n",
    "# Project directory\n",
    "topdir          {topdir}\n",
    "\n",
    "# Directories relative to topdir\n",
    "inputdir        {analysis_name}/DeepDive/Stage2\n",
    "outputdir       {analysis_name}/DeepDive/Stage3\n",
    "\"\"\"\n",
    "\n",
    "# This will save the ECF as a file that the next cell can read-in\n",
    "with open(f'./S3_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s3_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb5201-fc66-41e9-ba1e-a18d9a914813",
   "metadata": {},
   "source": [
    "### 3.2 Running Stage 3\n",
    "\n",
    "Here we run the Eureka! Stage 3 pipeline using the settings we defined above. For each aperture-annulus pair, this should take ~1 minute, but that will depend on the data volume of the observation you're working on and the specifics of your CPU.\n",
    "\n",
    "Given that we're considering many such pairs, **this will take a substantial amount of time**\n",
    "\n",
    "Unlike in the quick-look analysis, we're going to show the output plots as they're made (by having set s3_meta.hide_plots to False and having removed some of the Jupyter-notebook specific tweaks in the quick-look notebook). This might look a bit more cluttered, but it can be useful to examine each of the output figures as they're made to get a better understanding of what is happening and how your choices impact the reduction.\n",
    "\n",
    "If you previously ran Stage 3 and want to re-use those outputs, you can just comment-out the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc45e30-c515-424c-a9c4-c1d2575379fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if process:\n",
    "    spec, s3_meta = s3.reduce(eventlabel)\n",
    "else:\n",
    "    # If not running S3, automatically grab the latest MetaData and SpecData\n",
    "    temp_meta = MetaClass()\n",
    "    temp_meta.eventlabel = eventlabel\n",
    "    temp_meta.topdir = topdir\n",
    "    temp_meta.inputdir = f'{topdir}/{analysis_name}/DeepDive/Stage3/'\n",
    "    s3_meta, *_ = findevent(temp_meta, 'S3')\n",
    "    spec = xr.load_dataset(s3_meta.filename_S3_SpecData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc5d49-f7b4-403c-8cd5-a78bbd811daf",
   "metadata": {},
   "source": [
    "### 3.3 Investigating the Stage 3 outputs\n",
    "\n",
    "If there are issues with your analyses that you are having troubles working out, try re-running and increasing your `isplots_S3` setting to `5` to get more output plots that can sometimes be useful for debugging and/or increasing `nplots` to a larger number to investigate some of the repeated plots for more integrations or try setting `nplots` to `None` to make the repeated plots for all integrations (this will take a while and will make a *lot* of figures).\n",
    "\n",
    "Generally speaking, the lower the Stage 3 MAD (median-absolute deviation) value, the better the overall reduction is since the S3 MAD is approximately what the overall noise level is in the lightcurve. This is not always the case, as sometimes lightcurves with higher MAD values have cleaner correlated noise properties that can be better decorrelated in Stage 5. But generally speaking, the lower the S3 MAD value the better so long as that isn't coming at the cost of large amounts of data points being discarded. A decent first approach is to try different Stage 3 settings until you find a \"sweet-spot\", and then if you want to double-check your work and conclusions, you can run Stages 4-5 on different Stage 3 outputs to confirm that your lower-MAD reduction resulted in a \"better\" final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b32cb-161a-4197-bf2f-4ec047b81f4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Stage 4 - Removing time-series outliers\n",
    "\n",
    "Stage 4 for photometric data just clips outliers with respect to a box-car smoothed version of the signal - this helps remove any remaining outliers while also trying to avoid removing astrophysical signals.\n",
    "\n",
    "### 4.1 Setting up the Stage 4 ECF\n",
    "\n",
    "Parameters that might be worth varying are documented in the `README_DeepDive.md` file.\n",
    "\n",
    "By default, the most recently created Stage 3 output located in `s4_meta.inputdir` will be used as the input for your Stage 4. If you instead want to use a specific Stage 3 output, you'll need to update your `inputdir` setting to point to the specific Stage 3 output folder you want to work on (e.g., `'Stage3/S3_2025-02-28_trappist1b_eclipse4_run1'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec49be-a784-4b70-b6eb-1eb8808face6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4_ecf_contents = f\"\"\"# Eureka! Control File for Stage 4: Generate Lightcurves\n",
    "\n",
    "# Stage 4 Documentation: https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-4\n",
    "\n",
    "# Number of spectroscopic channels spread evenly over given wavelength range\n",
    "nspecchan       1\n",
    "compute_white   False\n",
    "\n",
    "allapers        True    # Run S4 on all of the apertures considered in S3? Otherwise will use newest output in the inputdir\n",
    "\n",
    "# Parameters for sigma clipping\n",
    "clip_binned     True    # Whether or not sigma clipping should be performed on the binned 1D time series\n",
    "sigma           3.5     # The number of sigmas a point must be from the rolling median to be considered an outlier\n",
    "box_width       20      # The width of the box-car filter (used to calculated the rolling median) in units of number of data points\n",
    "maxiters        20      # The number of iterations of sigma clipping that should be performed.\n",
    "boundary        fill    # Use 'fill' to extend the boundary values by the median of all data points (recommended), 'wrap' to use a periodic boundary, or 'extend' to use the first/last data points\n",
    "fill_value      mask    # Either the string 'mask' to mask the outlier values (recommended), 'boxcar' to replace data with the mean from the box-car filter, or a constant float-type fill value.\n",
    "\n",
    "# Project directory\n",
    "topdir          {topdir}\n",
    "\n",
    "# Directories relative to topdir\n",
    "inputdir        {analysis_name}/DeepDive/Stage3\n",
    "outputdir       {analysis_name}/DeepDive/Stage4\n",
    "\"\"\"\n",
    "\n",
    "# This will save the ECF as a file that the next cell can read-in\n",
    "with open(f'./S4_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s4_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16063d-08d8-461c-9151-e34c56800e93",
   "metadata": {},
   "source": [
    "### 4.2 Running Stage 4\n",
    "\n",
    "Here we run the Eureka! Stage 4 pipeline using the settings we defined above. For each aperture-annulus pair, this should take << 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79cd41-619c-4422-9533-db2b9dfe7de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if process:\n",
    "    spec, lc, s4_meta = s4.genlc(eventlabel)\n",
    "else:\n",
    "    # If not running S4, automatically grab the latest MetaData and SpecData\n",
    "    temp_meta = MetaClass()\n",
    "    temp_meta.eventlabel = eventlabel\n",
    "    temp_meta.topdir = topdir\n",
    "    temp_meta.inputdir = f'{topdir}/{analysis_name}/DeepDive/Stage4/'\n",
    "    s4_meta, path, _ = findevent(temp_meta, 'S4')\n",
    "    spec = xr.load_dataset(s4_meta.filename_S4_SpecData)\n",
    "    lc = xr.load_dataset(s4_meta.filename_S4_LCData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714600d-d0c9-4297-b370-89dd3af956f6",
   "metadata": {},
   "source": [
    "### 4.3 Investigating the Stage 4 outputs\n",
    "\n",
    "If there are still substantial outliers (think >5 sigma, since the size of the plotted error bars is likely a bit underestimated), you will need to adjust the `sigma` and `box_width` settings in your Stage 4 ECF in order to properly catch all the outliers. This might take a bit of guess-and-check work. Don't worry as much about outliers that are right at the very start of the obsevations though, as these will be removed in Stage 5 anyway when we're removing the worst of the detector settling effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0260efb-089a-433c-99f7-7a476ae413c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Stage 5 - Fitting the lightcurve\n",
    "\n",
    "### 5.1 Setting up the Stage 5 ECF\n",
    "\n",
    "Parameters that might be worth varying are documented in the `README_DeepDive.md` file. In particular, read the recommendations about the `allapers` setting at the top of the \"Context behind the Stage 5 ECF\" section.\n",
    "\n",
    "By default, the most recently created Stage 4 output located in `s5_meta.inputdir` will be used as the input for your Stage 5. If you instead want to use a specific Stage 4 output, you'll need to update your `inputdir` setting to point to the specific Stage 4 output folder you want to work on (e.g., `'Stage4/S4_2025-02-28_trappist1b_eclipse4_run1'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6f8a2-9936-4cde-9486-8718d8280fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_ecf_contents = f\"\"\"# Eureka! Control File for Stage 5: Lightcurve Fitting\n",
    "\n",
    "# Stage 5 Documentation: https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-5\n",
    "\n",
    "ncpu            16    # The number of CPU threads to use when running emcee or dynesty in parallel\n",
    "\n",
    "allapers        True  # Run S5 on all of the apertures considered in S4? Otherwise will use newest output in the inputdir\n",
    "\n",
    "fit_par         ./S5_{eventlabel}.epf\n",
    "fit_method      [dynesty]\n",
    "run_myfuncs     [batman_ecl, polynomial, expramp, xpos, ypos, xwidth, ywidth, GP]\n",
    "\n",
    "#GP inputs\n",
    "kernel_inputs   ['time']  # options: time\n",
    "kernel_class    ['Matern32']  # options: ExpSquared, Matern32, Exp, RationalQuadratic for george, Matern32 for celerite (sums of kernels possible for george separated by commas)\n",
    "GP_package      'celerite'  # options: george, celerite\n",
    "\n",
    "# Manual clipping in time\n",
    "manual_clip     [[None,50]]   # Remove the first ~50 integrations which will be most affected by detector settling\n",
    "\n",
    "# dynesty fitting parameters\n",
    "run_nlive       'min'         # Must be > ndim * (ndim + 1) // 2. Use 'min' to use the minimum safe number\n",
    "run_bound       'multi'\n",
    "run_sample      'rwalk'\n",
    "run_tol         0.01\n",
    "\n",
    "# Plotting controls\n",
    "interp          True    # Should astrophysical model be interpolated (useful for uneven sampling like that from HST)\n",
    "\n",
    "# Diagnostics\n",
    "isplots_S5      5       # Generate few (1), some (3), or many (5) figures (Options: 1 - 5)\n",
    "\n",
    "# Project directory\n",
    "topdir          {topdir}\n",
    "\n",
    "# Directories relative to topdir\n",
    "inputdir        {analysis_name}/DeepDive/Stage4\n",
    "outputdir       {analysis_name}/DeepDive/Stage5\n",
    "\"\"\"\n",
    "\n",
    "# This will save the ECF as a file that the next cell can read-in\n",
    "with open(f'./S5_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s5_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd644e5-cc80-422e-ab9e-6b4e71f35b49",
   "metadata": {},
   "source": [
    "### 5.2 Setting up the Stage 5 EPF\n",
    "\n",
    "A description of how you should adjust your ECF parameters is in the `README_DeepDive.md` file.\n",
    "\n",
    "Make sure to update the astrophysical parameter priors based on those provided in the relevant Jira ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05702e0b-7544-42a5-9cfb-31faee00051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_epf = \"\"\"\n",
    "# Stage 5 Fit Parameters Documentation: https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-5-fit-parameters\n",
    "\n",
    "#Name         Value                 Free?            PriorPar1        PriorPar2    PriorType\n",
    "# \"Free?\" can be free, fixed, white_free, white_fixed, shared, or independent\n",
    "# PriorType can be U (Uniform), LU (Log Uniform), or N (Normal).\n",
    "# If U/LU, PriorPar1 and PriorPar2 represent lower and upper limits of the parameter/log(the parameter).\n",
    "# If N, PriorPar1 is the mean and PriorPar2 is the standard deviation of a Gaussian prior.\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "rp          YourRadiusHere      'fixed'\n",
    "fp          YourFpHere          'free'           YourFpHere    2000e-6     N\n",
    "# ------------------\n",
    "# Orbital parameters\n",
    "# ------------------\n",
    "per         YourPerHere         'fixed'\n",
    "t_secondary YourTsecHere        'free'           YourTsecHere  YourTsecUncertHere N\n",
    "inc         YourIncHere         'fixed'\n",
    "a           YourAHere           'fixed'\n",
    "ecc         0.                  'fixed'\n",
    "w           90.                 'fixed'\n",
    "# The following two lines are commented out, but you can uncomment them (while commenting out the ecc and w lines above) and edit them if needed for your planet\n",
    "# ecosw       YourEcoswHere       'fixed'          YourEcoswHere YourEcoswUncertHere N\n",
    "# esinw       YourEsinwHere       'fixed'          YourEsinwHere YourEsinwUncertHere N\n",
    "# --------------------------------------------------------------------------\n",
    "# Systematic variables (these can be left as-is for the Quick-Look analysis)\n",
    "# --------------------------------------------------------------------------\n",
    "# Polynomial Parameters\n",
    "c0          0.999               'free'           0.999         0.01        N\n",
    "c1          -0.002              'free'           0.0           0.1         N\n",
    "# Ramp Parameters\n",
    "r0          0.002               'free'           0.0           0.01        N\n",
    "r1          50                  'free'           3             300         U\n",
    "# Centroid decorrelation parameters\n",
    "ypos        0.0                 'free'           0.0           0.5         N\n",
    "xpos        0.0                 'free'           0.0           0.5         N\n",
    "ywidth      0.0                 'free'           0.0           0.5         N\n",
    "xwidth      0.0                 'free'           0.0           0.5         N\n",
    "# Gaussian Process parameters (only used if GP added to s5_meta.run_myfuncs and uncommented below)\n",
    "A           -17                 'free'          -24            -10         U\n",
    "m           -4                  'free'          -7             0           U\n",
    "# White noise\n",
    "scatter_mult 1.4                'free'           0.8           10          U\n",
    "\"\"\"\n",
    "\n",
    "# This code will write your EPF settings to a file and doesn't need to be changed\n",
    "epf_filename = f'./S5_{eventlabel}.epf'\n",
    "with open(epf_filename, 'w') as file:\n",
    "    file.writelines(s5_epf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc541bf-13f8-44c6-bec8-c173d6c4c206",
   "metadata": {},
   "source": [
    "### 5.3 Running Stage 5\n",
    "\n",
    "Here we run the Eureka! Stage 5 pipeline using the settings we defined above. This should take ~1 minute or less per fit, but that will depend on the data volume of the observation you're working on, the specifics of your CPU, and how well the model matches your data.\n",
    "\n",
    "Again, if you've set `allapers` to `True`, then this stage will take **quite a long time** in total.\n",
    "\n",
    "Note, if you re-run Stage 5 without restarting the Jupyter notebook, the color of your plot will change each time you run Stage 5. This is not an issue and there's no need to restart the notebook; it is just a consequence of the fact that Eureka! was not originally intended to be run within Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb9387-aba0-4fad-afa6-3d8cbb5d250c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if process:\n",
    "    s5_meta = s5.fitlc(eventlabel)\n",
    "else:\n",
    "    # If not running S5, automatically grab the latest MetaData\n",
    "    temp_meta = MetaClass()\n",
    "    temp_meta.eventlabel = eventlabel\n",
    "    temp_meta.topdir = topdir\n",
    "    temp_meta.inputdir = f'{topdir}/{analysis_name}/DeepDive/Stage5/'\n",
    "    s5_meta, *_ = findevent(temp_meta, 'S5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936e816-6047-4833-b1b6-172df787add2",
   "metadata": {},
   "source": [
    "---\n",
    "Only proceed to the cells below once you've run your suite of Stage 5 fits with `allapers` set to `True`\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Comparing results for different aperture sizes\n",
    "\n",
    "### 6.1 Choosing a initial \"fiducial\"/\"best\" reduction\n",
    "\n",
    "To start, the following plotting code will put a bit of focus on the reduction which produces the lowest eclipse-depth uncertainty. This is often a fairly reasonable choice, but make sure to use the following plots to investigate that choice and look for any potential biases or items of concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d427ae-000f-4c8a-969d-9b52dd085ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitted eclipse depth uncertainty for every considered aperture+annulus pairing\n",
    "aps = []\n",
    "bgs = []\n",
    "eclipseDepthError = []\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        aps.append(ap)\n",
    "        bgs.append(bg)\n",
    "        eclipseDepthError.append(np.std(samples.fp.values)*1e6)\n",
    "aps = np.array(aps)\n",
    "bgs = np.array(bgs)\n",
    "eclipseDepthError = np.array(eclipseDepthError)\n",
    "\n",
    "best_ap = aps[np.argmin(eclipseDepthError)]\n",
    "best_bg = bgs[np.argmin(eclipseDepthError)]\n",
    "\n",
    "print(f'The automatically recommended fiducial reduction aperture+annulus pair is: ap{best_ap}_bg{best_bg}')\n",
    "print('Be sure not to blindly trust this recommendation and investigate the plots below!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4436e-c8a8-4fcb-9698-6b57213cd86d",
   "metadata": {},
   "source": [
    "#### Pulling-up the S5 plots for the initially recommended aperture+annulus pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bc2a9-be8b-4728-9596-296ffe120083",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = np.sort(glob(f'{s5_meta.outputdir}../ap{best_ap}_bg{best_bg}/figs/*'))\n",
    "for figure in figures:\n",
    "    if 'startingpoint' in figure.lower():\n",
    "        continue\n",
    "    display(Image(filename=figure, embed=True, width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250b42a-9706-40ff-8dff-1c6e51834f1f",
   "metadata": {},
   "source": [
    "Do the above plots all look good to you? Does the model do a good job of fitting the data, does the residual lightcurve look quite flat and centered at 0 ppm, does the GP model (if fitted) look smooth and not like it was trying to fit super high-frequency white noise, does the empirical line on the Allan variance plot look quite close to the predicted trend, does the histogram of residuals look Gaussian with the same approximate standard deviation as expected, and does the corner plot look fairly converged and does it show any concerning correlations between the fitted eclipse depth or eclipse timing and any of the systematic noise parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386699b-6d92-4379-8937-2d98e4fade15",
   "metadata": {},
   "source": [
    "### 6.2 Comparing lightcurve noise levels vs aperture properties (rough gauge for \"best\" reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87469ae5-60f3-4086-8988-58e0e8daf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        if ap == s3_meta.spec_hw_range[0] and bg == s3_meta.bg_hw_range[0]:\n",
    "            label2 = 'S4 MAD'\n",
    "            label3 = 'S5 Scatter'\n",
    "        else:\n",
    "            label2 = None\n",
    "            label3 = None\n",
    "\n",
    "        meta_s4_temp = eureka.lib.manageevent.loadevent(f'{s4_meta.outputdir}../ap{ap}_bg{bg}/S4_{eventlabel}_Meta_Save.dat')\n",
    "        ax.plot(ap, meta_s4_temp.mad_s4, '*', c='C1', label=label2)\n",
    "        \n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        fit = pd.read_csv(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_{eventlabel}_ap{ap}_bg{bg}_Table_Save_ch0.txt', delim_whitespace=True, comment='#')\n",
    "        ax.errorbar(ap, np.nanmedian(fit.lcerr)*1e6, np.nanmedian(fit.lcerr)*1e6/np.median(samples.scatter_mult)*np.std(samples.scatter_mult), fmt='.', color='C2', label=label3)\n",
    "        \n",
    "ax.set_ylabel('Noise Level (ppm)')\n",
    "ax.set_xlabel('Aperture Radius (px)')\n",
    "ax.legend(loc='best')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817807d5-0fb9-4a88-987d-c79545e74c1e",
   "metadata": {},
   "source": [
    "Is there any obvious preference for a particular aperture radius?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfaf9c-939e-4143-9052-ef7b1b8d4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        if ap == s3_meta.spec_hw_range[0] and bg == s3_meta.bg_hw_range[0]:\n",
    "            label2 = 'S4 MAD'\n",
    "            label3 = 'S5 Scatter'\n",
    "        else:\n",
    "            label2 = None\n",
    "            label3 = None\n",
    "\n",
    "        meta_s4_temp = eureka.lib.manageevent.loadevent(f'{s4_meta.outputdir}../ap{ap}_bg{bg}/S4_{eventlabel}_Meta_Save.dat')\n",
    "        ax.plot(bg.split('_')[0], meta_s4_temp.mad_s4, '*', c='C1', label=label2)\n",
    "        \n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        fit = pd.read_csv(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_{eventlabel}_ap{ap}_bg{bg}_Table_Save_ch0.txt', delim_whitespace=True, comment='#')\n",
    "        ax.errorbar(bg.split('_')[0], np.nanmedian(fit.lcerr)*1e6, np.nanmedian(fit.lcerr)*1e6/np.median(samples.scatter_mult)*np.std(samples.scatter_mult), fmt='.', color='C2', label=label3)\n",
    "        \n",
    "ax.set_ylabel('Noise Level (ppm)')\n",
    "ax.set_xlabel('Inner Annulus Radius (px)')\n",
    "ax.legend(loc='best')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e960594-7fbe-445f-b539-f95537d5894f",
   "metadata": {},
   "source": [
    "Is there any obvious preference toward a certain annulus inner radius?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e1fec-2ef6-48c2-90f9-48241535763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        if ap == s3_meta.spec_hw_range[0] and bg == s3_meta.bg_hw_range[0]:\n",
    "            label2 = 'S4 MAD'\n",
    "            label3 = 'S5 Scatter'\n",
    "        else:\n",
    "            label2 = None\n",
    "            label3 = None\n",
    "\n",
    "        meta_s4_temp = eureka.lib.manageevent.loadevent(f'{s4_meta.outputdir}../ap{ap}_bg{bg}/S4_{eventlabel}_Meta_Save.dat')\n",
    "        ax.plot(np.diff(np.array(bg.split('_')).astype(int)), meta_s4_temp.mad_s4, '*', c='C1', label=label2)\n",
    "        \n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        fit = pd.read_csv(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_{eventlabel}_ap{ap}_bg{bg}_Table_Save_ch0.txt', delim_whitespace=True, comment='#')\n",
    "        ax.errorbar(np.diff(np.array(bg.split('_')).astype(int)), np.nanmedian(fit.lcerr)*1e6, np.nanmedian(fit.lcerr)*1e6/np.median(samples.scatter_mult)*np.std(samples.scatter_mult), fmt='.', color='C2', label=label3)\n",
    "        \n",
    "ax.set_ylabel('Noise Level (ppm)')\n",
    "ax.set_xlabel('Annulus Width (px)')\n",
    "ax.legend(loc='best')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ec48b-15f1-42d4-b905-8b403b7f59f6",
   "metadata": {},
   "source": [
    "Is there any obvious preference toward a certain annulus width?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc6f38-5ef6-479a-b5ca-ca93fec011c3",
   "metadata": {},
   "source": [
    "### 6.3 Comparing fitted eclipse depth uncertainties vs aperture properties (a slightly better, but slightly riskier, gauge for \"best\" reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf53ca-0ea3-46a6-b178-88e3855b7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        ax.errorbar(ap, np.std(samples.fp)*1e6, fmt='.', color='k')\n",
    "\n",
    "ax.set_ylabel('Fitted Eclipse Depth Uncertainty (ppm)')\n",
    "ax.set_xlabel('Aperture Radius (px)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5d13a-b936-4451-931f-67efb4dc7304",
   "metadata": {},
   "source": [
    "Is there any obvious preference for a particular aperture radius?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6eca8-f095-4ea6-99f3-901f2f9e14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        ax.errorbar(bg.split('_')[0], np.std(samples.fp)*1e6, fmt='.', color='k')\n",
    "\n",
    "ax.set_ylabel('Fitted Eclipse Depth Uncertainty (ppm)')\n",
    "ax.set_xlabel('Inner Annulus Radius (px)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57f2cc-f050-4c96-8474-f31b37b5ab8f",
   "metadata": {},
   "source": [
    "Is there any obvious preference toward a certain annulus inner radius?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70a9f0-dd07-4933-b03c-5fa9c8950229",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in [best_ap,]:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        ax.errorbar(np.diff(np.array(bg.split('_')).astype(int)), np.std(samples.fp)*1e6, fmt='.', color='k')\n",
    "\n",
    "ax.set_ylabel('Fitted Eclipse Depth Uncertainty (ppm)')\n",
    "ax.set_xlabel('Annulus Width (px)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204b715-cc78-4c30-9cb7-ba103c394934",
   "metadata": {},
   "source": [
    "Is there any obvious preference toward a certain annulus width?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e0651-e59d-45ea-a57f-4444da84a55a",
   "metadata": {},
   "source": [
    "### 6.4 Comparing fitted eclipse depth vs aperture properties (looking for possible biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c20065-c9ec-4dae-9d32-a6b29d89fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        ax.errorbar(ap, np.median(samples.fp)*1e6, np.std(samples.fp)*1e6, fmt='.', color='k')\n",
    "\n",
    "ax.set_ylabel('Fitted Eclipse Depth (ppm)')\n",
    "ax.set_xlabel('Aperture Radius (px)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b49c98-1b2c-4ac8-b4dc-55790e369e88",
   "metadata": {},
   "source": [
    "Does the above plot suggest there is any obvious bias caused by the choice of aperture radius?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d62031-b1de-42e7-b533-1555af0315aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        ax.errorbar(bg.split('_')[0], np.median(samples.fp)*1e6, np.std(samples.fp)*1e6, fmt='.', color='k')\n",
    "\n",
    "ax.set_ylabel('Fitted Eclipse Depth (ppm)')\n",
    "ax.set_xlabel('Inner Annulus Radius (px)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace42714-151b-4ff5-9a9e-a62b62e74aa6",
   "metadata": {},
   "source": [
    "Does the above plot suggest there is any obvious bias caused by the choice of inner radius of the background annulus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6710a3c-f57a-4476-b496-06040ae1cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, num=0)\n",
    "\n",
    "for ap in s3_meta.spec_hw_range:\n",
    "    for bg in s3_meta.bg_hw_range:\n",
    "        samples = xr.load_dataset(f'{s5_meta.outputdir}../ap{ap}_bg{bg}/S5_dynesty_samples_ch0.h5', engine='h5netcdf')\n",
    "        ax.errorbar(np.diff(np.array(bg.split('_')).astype(int)), np.median(samples.fp)*1e6, np.std(samples.fp)*1e6, fmt='.', color='k')\n",
    "\n",
    "ax.set_ylabel('Fitted Eclipse Depth (ppm)')\n",
    "ax.set_xlabel('Annulus Width (px)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3c60f-effd-4c71-b0c3-ab8cb773abbe",
   "metadata": {},
   "source": [
    "Does the above plot suggest there is any obvious bias caused by the choice of the width of the background annulus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7d615-e2a9-4684-b39b-6b45e85ac1c4",
   "metadata": {},
   "source": [
    "## 7. Choosing a final fiducial reduction\n",
    "\n",
    "If you feel the need to change the \"fiducial\" aperture+annulus radii from the initially suggested values, you can do any extra work needed in the following cells to work out what you think should be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6240c-13bb-4030-88a5-95859298fb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adda52d-4fd8-4a46-a582-d19680cb5f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52019a78-f898-46de-bc96-6521206dfbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29515b6-e0b8-4914-921f-b690ab4f34f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958ee70-d6eb-4c2d-99fa-9b755c22bc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fb3b2-65fe-45dc-bb40-1c3e609feb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eureka_rwddt] *",
   "language": "python",
   "name": "conda-env-eureka_rwddt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
